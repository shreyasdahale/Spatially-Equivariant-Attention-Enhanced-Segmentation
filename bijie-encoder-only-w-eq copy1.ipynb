{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10334130,"sourceType":"datasetVersion","datasetId":6398811}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install escnn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T04:53:56.313807Z","iopub.execute_input":"2025-01-12T04:53:56.314181Z","iopub.status.idle":"2025-01-12T04:54:22.557261Z","shell.execute_reply.started":"2025-01-12T04:53:56.314151Z","shell.execute_reply":"2025-01-12T04:54:22.556025Z"}},"outputs":[{"name":"stdout","text":"Collecting escnn\n  Downloading escnn-1.0.11-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from escnn) (2.4.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from escnn) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from escnn) (1.13.1)\nCollecting lie-learn (from escnn)\n  Downloading lie_learn-0.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from escnn) (1.4.2)\nCollecting pymanopt (from escnn)\n  Downloading pymanopt-2.2.1-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from escnn) (1.7.0)\nCollecting py3nj (from escnn)\n  Downloading py3nj-0.2.1.tar.gz (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (2024.6.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lie-learn->escnn) (2.32.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->escnn) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->lie-learn->escnn) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lie-learn->escnn) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->lie-learn->escnn) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->lie-learn->escnn) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->escnn) (1.3.0)\nDownloading escnn-1.0.11-py3-none-any.whl (373 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.9/373.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lie_learn-0.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pymanopt-2.2.1-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: py3nj\n  Building wheel for py3nj (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for py3nj: filename=py3nj-0.2.1-cp310-cp310-linux_x86_64.whl size=44764 sha256=76df1d02a853ffd71ecf73a600613eb4a1e0dbe82e4b18d12d4015b0ca72ef19\n  Stored in directory: /root/.cache/pip/wheels/71/e9/70/30a34ed6dbc8b54ce93f25c091be4cf7a24319e27d953a882b\nSuccessfully built py3nj\nInstalling collected packages: py3nj, pymanopt, lie-learn, escnn\nSuccessfully installed escnn-1.0.11 lie-learn-0.0.2 py3nj-0.2.1 pymanopt-2.2.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom escnn.gspaces import flipRot2dOnR2\nfrom escnn.nn import FieldType, R2Conv, NormBatchNorm, ReLU, SequentialModule, EquivariantModule, FieldType, GeometricTensor, PointwiseNonLinearity, InnerBatchNorm\nfrom PIL import Image, ImageOps\nfrom skimage.util import random_noise\nfrom skimage import exposure\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T04:55:25.853804Z","iopub.execute_input":"2025-01-12T04:55:25.854170Z","iopub.status.idle":"2025-01-12T04:55:42.309315Z","shell.execute_reply.started":"2025-01-12T04:55:25.854140Z","shell.execute_reply":"2025-01-12T04:55:42.307979Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"`Helper functions`","metadata":{}},{"cell_type":"code","source":"def load_image(image_path, target_size):\n    image = Image.open(image_path).convert('RGB' if target_size[2] == 3 else 'L')\n    transform = transforms.Compose([\n        transforms.Resize(target_size[:2]),\n    ])\n    return transform(image)\n\ndef load_real_data(data_dir, target_size=(256, 256)):\n    landslide_dir = os.path.join(data_dir, 'landslide')\n    non_landslide_dir = os.path.join(data_dir, 'non-landslide')\n\n    images = []\n    dems = []\n    labels = []\n\n    def process_image(image_path, dem_path, label):\n        image = load_image(image_path, target_size + (3,))\n        dem = load_image(dem_path, target_size + (1,))\n\n        images.append(transforms.ToTensor()(image))\n        dems.append(transforms.ToTensor()(dem))\n        labels.append(label)\n\n    for filename in os.listdir(os.path.join(landslide_dir, 'image')):\n        if filename.endswith(\".png\"):\n            image_path = os.path.join(landslide_dir, 'image', filename)\n            dem_path = os.path.join(landslide_dir, 'dem', filename)\n            process_image(image_path, dem_path, label=1)  # Label 1 for landslide\n\n    for filename in os.listdir(os.path.join(non_landslide_dir, 'image')):\n        if filename.endswith(\".png\"):\n            image_path = os.path.join(non_landslide_dir, 'image', filename)\n            dem_path = os.path.join(non_landslide_dir, 'dem', filename)\n            process_image(image_path, dem_path, label=0)  # Label 0 for non-landslide\n\n    images = torch.stack(images)\n    dems = torch.stack(dems)\n    labels = torch.tensor(labels, dtype=torch.float32)\n\n    return images, dems, labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T11:06:41.094336Z","iopub.execute_input":"2025-01-11T11:06:41.094898Z","iopub.status.idle":"2025-01-11T11:06:41.102582Z","shell.execute_reply.started":"2025-01-11T11:06:41.094867Z","shell.execute_reply":"2025-01-11T11:06:41.101695Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"`Network components`","metadata":{}},{"cell_type":"code","source":"class ConvBlock(EquivariantModule):\n    def __init__(self, space, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n\n        # Define input and output types\n        self.space = space\n        group_size = len(self.gspace.fibergroup.elements)\n        \n        self.in_type = FieldType(self.space, [self.space.regular_repr] * (in_channels // group_size))\n        self.out_type = FieldType(self.space, [self.space.regular_repr] * (in_channels // group_size))\n\n        self.conv = SequentialModule(\n            R2Conv(self.in_type, self.out_type, kernel_size=3, padding=1, bias=False),\n            InnerBatchNorm(self.out_type),\n            ReLU(self.out_type),\n            R2Conv(self.out_type, self.out_type, kernel_size=3, padding=1, bias=False),\n            InnerBatchNorm(self.out_type),\n            ReLU(self.out_type)\n        )\n\n    def forward(self, x):\n        if not isinstance(x, GeometricTensor):\n            geo_x = GeometricTensor(x, self.in_type)\n        else:\n            geo_x = x\n        return self.conv(geo_x).tensor\n\n    def evaluate_output_shape(self, input_shape):\n        return self.conv.evaluate_output_shape(input_shape)\n\n    # def export(self):\n    #     raise NotImplementedError(\"Exporting to standard PyTorch not implemented yet\")\n\nclass Encoder(EquivariantModule):\n    def __init__(self):\n        super(Encoder, self).__init__()\n\n        # Define the G-space for P4m equivariance\n        self.space = flipRot2dOnR2(4)  # 4 rotations + flips\n\n        self.conv1_rgb = ConvBlock(self.space, 3, 16)\n        self.conv1_dem = ConvBlock(self.space, 1, 8)\n        self.conv2 = ConvBlock(self.space, 24, 32)\n        self.conv3 = ConvBlock(self.space, 32, 64)\n        self.conv4 = ConvBlock(self.space, 64, 128)\n        self.bottleneck = ConvBlock(self.space, 128, 256)\n\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        # Fully connected layer without equivariance\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x_rgb, x_dem):\n        # x_rgb = GeometricTensor(x_rgb, self.conv1_rgb.in_type)\n        # x_dem = GeometricTensor(x_dem, self.conv1_dem.in_type)\n\n        c1_rgb = self.conv1_rgb(x_rgb)\n        c1_dem = self.conv1_dem(x_dem)\n\n        combined = torch.cat([c1_rgb.tensor, c1_dem.tensor], dim=1)\n        # combined = GeometricTensor(combined, self.conv2.in_type)\n\n        c2 = self.conv2(combined)\n        p2 = nn.MaxPool2d(kernel_size=2)(c2.tensor)\n        # p2 = GeometricTensor(p2, self.conv3.in_type)\n\n        c3 = self.conv3(p2)\n        p3 = nn.MaxPool2d(kernel_size=2)(c3.tensor)\n        # p3 = GeometricTensor(p3, self.conv4.in_type)\n\n        c4 = self.conv4(p3)\n        p4 = nn.MaxPool2d(kernel_size=2)(c4.tensor)\n        # p4 = GeometricTensor(p4, self.bottleneck.in_type)\n\n        bn = self.bottleneck(p4)\n        pooled = self.global_pool(bn.tensor)\n\n        return self.fc(pooled)\n\n    def evaluate_output_shape(self, input_shape):\n        raise NotImplementedError(\"Shape evaluation not implemented yet\")\n\n    # def export(self):\n    #     raise NotImplementedError(\"Exporting to standard PyTorch not implemented yet\")\n\n# Usage example\nencoder = Encoder()\nx_rgb = torch.randn(1, 3, 256, 256)  # Input for RGB\nx_dem = torch.randn(1, 1, 256, 256)  # Input for DEM\noutput = encoder(x_rgb, x_dem)\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T04:59:01.759779Z","iopub.execute_input":"2025-01-12T04:59:01.760137Z","iopub.status.idle":"2025-01-12T05:00:02.734823Z","shell.execute_reply.started":"2025-01-12T04:59:01.760109Z","shell.execute_reply":"2025-01-12T05:00:02.733098Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-fc2dd0eddb8e>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mx_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Input for RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mx_dem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Input for DEM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-fc2dd0eddb8e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_rgb, x_dem)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mx_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeometricTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_rgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mx_dem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeometricTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_dem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/escnn/nn/geometric_tensor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor, type, coords)\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0;34mf\"Error! Points in the `tensor` and `coords` tensors do not match: {tensor.shape[0]} != {coords.shape[0]}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;34mf\"Error! The size of the tensor {tensor.shape} does not match the size of the field type {type.size}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Error! The size of the tensor torch.Size([1, 3, 256, 256]) does not match the size of the field type 24."],"ename":"AssertionError","evalue":"Error! The size of the tensor torch.Size([1, 3, 256, 256]) does not match the size of the field type 24.","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T11:48:57.266078Z","iopub.execute_input":"2025-01-08T11:48:57.266296Z","iopub.status.idle":"2025-01-08T11:48:57.330331Z","shell.execute_reply.started":"2025-01-08T11:48:57.266268Z","shell.execute_reply":"2025-01-08T11:48:57.329357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchinfo import summary\n\nmodel = Encoder().to(device)\nsummary(model, input_size=[(1, 3, 256, 256), (1, 1, 256, 256)]) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = '/kaggle/input/bijie/Bijie_dataset'\nimages, dems, labels = load_real_data(data_dir, target_size=(256, 256))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T11:48:57.331365Z","iopub.execute_input":"2025-01-08T11:48:57.331721Z","iopub.status.idle":"2025-01-08T11:49:59.224459Z","shell.execute_reply.started":"2025-01-08T11:48:57.331665Z","shell.execute_reply":"2025-01-08T11:49:59.223785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(dems)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T11:50:18.166835Z","iopub.execute_input":"2025-01-08T11:50:18.167180Z","iopub.status.idle":"2025-01-08T11:50:18.173165Z","shell.execute_reply.started":"2025-01-08T11:50:18.167151Z","shell.execute_reply":"2025-01-08T11:50:18.172298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_img, X_test_img, X_train_dem, X_test_dem, y_train, y_test = train_test_split(\n    images, dems, labels, test_size=0.2, random_state=42\n)\n\nprint(X_train_img.shape, X_train_dem[0].shape, y_train[0].shape)\nprint(\"Training data size:\", len(X_train_img))\nprint(\"Testing data size:\", len(X_test_img))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T11:50:42.081924Z","iopub.execute_input":"2025-01-08T11:50:42.082288Z","iopub.status.idle":"2025-01-08T11:50:44.196851Z","shell.execute_reply.started":"2025-01-08T11:50:42.082259Z","shell.execute_reply":"2025-01-08T11:50:44.196065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, images, dems, masks):\n        self.images = images\n        self.dems = dems\n        self.masks = masks\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        return {\n            'image': self.images[idx],\n            'dem': self.dems[idx],\n            'label': self.masks[idx]\n        }\n\ntrain_dataset = CustomDataset(X_train_img, X_train_dem, y_train)\nval_dataset = CustomDataset(X_test_img, X_test_dem, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T11:55:23.296917Z","iopub.execute_input":"2025-01-08T11:55:23.297211Z","iopub.status.idle":"2025-01-08T11:55:23.303126Z","shell.execute_reply.started":"2025-01-08T11:55:23.297190Z","shell.execute_reply":"2025-01-08T11:55:23.302109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Encoder().to(device)\nepochs = 200\nbest_val_loss = float('inf')\nbest_model_path = 'best_unet_model.pth'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:03:57.662797Z","iopub.execute_input":"2025-01-08T12:03:57.663114Z","iopub.status.idle":"2025-01-08T12:03:57.683326Z","shell.execute_reply.started":"2025-01-08T12:03:57.663088Z","shell.execute_reply":"2025-01-08T12:03:57.682618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.MSELoss()\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n\n    for batch in train_loader:\n        images = batch['image'].to(device)\n        dems = batch['dem'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images, dems).squeeze(1) \n\n        binary_preds = (outputs > 0.5).float()\n\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    train_loss /= len(train_loader)\n\n    model.eval()\n    val_loss = 0.0\n    all_preds = []\n    all_targets = []\n    with torch.no_grad():\n        for batch in val_loader:\n            images = batch['image'].to(device)\n            dems = batch['dem'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(images, dems).squeeze(1)\n\n            binary_preds = (outputs > 0.5).float()\n\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            preds = binary_preds.cpu().numpy().astype(int)\n            targets = labels.cpu().numpy().astype(int)\n            all_preds.append(preds)\n            all_targets.append(targets)\n\n    val_loss /= len(val_loader)\n\n    all_preds = np.concatenate([pred.flatten() for pred in all_preds])\n    all_targets = np.concatenate([target.flatten() for target in all_targets])\n\n    accuracy = accuracy_score(all_targets, all_preds)\n    precision = precision_score(all_targets, all_preds, zero_division=0)\n    recall = recall_score(all_targets, all_preds, zero_division=0)\n    f1 = f1_score(all_targets, all_preds)\n    iou = jaccard_score(all_targets, all_preds)\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, IoU: {iou}, F1: {f1}\")\n\n    scheduler.step(val_loss)\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"Saved best model with val loss: {best_val_loss}\")\n\nprint(\"Training complete. Best model saved to\", best_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:14:12.325733Z","iopub.execute_input":"2025-01-08T12:14:12.326070Z","execution_failed":"2025-01-08T12:16:46.007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}