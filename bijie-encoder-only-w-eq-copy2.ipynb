{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10334130,"sourceType":"datasetVersion","datasetId":6398811}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install escnn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T05:18:31.045737Z","iopub.execute_input":"2025-01-12T05:18:31.046136Z","iopub.status.idle":"2025-01-12T05:18:58.229069Z","shell.execute_reply.started":"2025-01-12T05:18:31.046106Z","shell.execute_reply":"2025-01-12T05:18:58.227384Z"}},"outputs":[{"name":"stdout","text":"Collecting escnn\n  Downloading escnn-1.0.11-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from escnn) (2.4.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from escnn) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from escnn) (1.13.1)\nCollecting lie-learn (from escnn)\n  Downloading lie_learn-0.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from escnn) (1.4.2)\nCollecting pymanopt (from escnn)\n  Downloading pymanopt-2.2.1-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from escnn) (1.7.0)\nCollecting py3nj (from escnn)\n  Downloading py3nj-0.2.1.tar.gz (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (2024.6.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lie-learn->escnn) (2.32.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->escnn) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->lie-learn->escnn) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lie-learn->escnn) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->lie-learn->escnn) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->lie-learn->escnn) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->escnn) (1.3.0)\nDownloading escnn-1.0.11-py3-none-any.whl (373 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.9/373.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading lie_learn-0.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pymanopt-2.2.1-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: py3nj\n  Building wheel for py3nj (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for py3nj: filename=py3nj-0.2.1-cp310-cp310-linux_x86_64.whl size=44764 sha256=c9fd1e48e790b6bc71b4272ac3d4ffbb2200c17b535a3329469211b482896ba7\n  Stored in directory: /root/.cache/pip/wheels/71/e9/70/30a34ed6dbc8b54ce93f25c091be4cf7a24319e27d953a882b\nSuccessfully built py3nj\nInstalling collected packages: py3nj, pymanopt, lie-learn, escnn\nSuccessfully installed escnn-1.0.11 lie-learn-0.0.2 py3nj-0.2.1 pymanopt-2.2.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom escnn.gspaces import flipRot2dOnR2\nfrom escnn.nn import FieldType, R2Conv, NormBatchNorm, ReLU, SequentialModule, EquivariantModule, FieldType, GeometricTensor, PointwiseNonLinearity, InnerBatchNorm\nfrom PIL import Image, ImageOps\nfrom skimage.util import random_noise\nfrom skimage import exposure\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T05:19:45.663156Z","iopub.execute_input":"2025-01-12T05:19:45.663557Z","iopub.status.idle":"2025-01-12T05:20:00.153066Z","shell.execute_reply.started":"2025-01-12T05:19:45.663527Z","shell.execute_reply":"2025-01-12T05:20:00.151836Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"`Helper functions`","metadata":{}},{"cell_type":"code","source":"def load_image(image_path, target_size):\n    image = Image.open(image_path).convert('RGB' if target_size[2] == 3 else 'L')\n    transform = transforms.Compose([\n        transforms.Resize(target_size[:2]),\n    ])\n    return transform(image)\n\ndef load_real_data(data_dir, target_size=(256, 256)):\n    landslide_dir = os.path.join(data_dir, 'landslide')\n    non_landslide_dir = os.path.join(data_dir, 'non-landslide')\n\n    images = []\n    dems = []\n    labels = []\n\n    def process_image(image_path, dem_path, label):\n        image = load_image(image_path, target_size + (3,))\n        dem = load_image(dem_path, target_size + (1,))\n\n        images.append(transforms.ToTensor()(image))\n        dems.append(transforms.ToTensor()(dem))\n        labels.append(label)\n\n    for filename in os.listdir(os.path.join(landslide_dir, 'image')):\n        if filename.endswith(\".png\"):\n            image_path = os.path.join(landslide_dir, 'image', filename)\n            dem_path = os.path.join(landslide_dir, 'dem', filename)\n            process_image(image_path, dem_path, label=1)\n\n    for filename in os.listdir(os.path.join(non_landslide_dir, 'image')):\n        if filename.endswith(\".png\"):\n            image_path = os.path.join(non_landslide_dir, 'image', filename)\n            dem_path = os.path.join(non_landslide_dir, 'dem', filename)\n            process_image(image_path, dem_path, label=0)\n\n    images = torch.stack(images)\n    dems = torch.stack(dems)\n    labels = torch.tensor(labels, dtype=torch.float32)\n\n    return images, dems, labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T11:06:41.094336Z","iopub.execute_input":"2025-01-11T11:06:41.094898Z","iopub.status.idle":"2025-01-11T11:06:41.102582Z","shell.execute_reply.started":"2025-01-11T11:06:41.094867Z","shell.execute_reply":"2025-01-11T11:06:41.101695Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"`Network components`","metadata":{}},{"cell_type":"code","source":"class ConvBlock1(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ConvBlock1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass ConvBlock(EquivariantModule):\n    def __init__(self, space, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n        self.space = space\n        group_size = len(self.space.fibergroup.elements)\n        self.in_type = FieldType(self.space, [self.space.regular_repr] * (in_channels // group_size))\n        self.out_type = FieldType(self.space, [self.space.regular_repr] * (out_channels // group_size))\n\n        self.conv = SequentialModule(\n            R2Conv(self.in_type, self.out_type, kernel_size=3, padding=1, bias=False),\n            InnerBatchNorm(self.out_type),\n            ReLU(self.out_type),\n            R2Conv(self.out_type, self.out_type, kernel_size=3, padding=1, bias=False),\n            InnerBatchNorm(self.out_type),\n            ReLU(self.out_type)\n        )\n\n    def forward(self, x):\n        if not isinstance(x, GeometricTensor):\n            geo_x = GeometricTensor(x, self.in_type)\n        else:\n            geo_x = x\n        return self.conv(geo_x).tensor\n\n    def evaluate_output_shape(self, input_shape):\n        return self.conv.evaluate_output_shape(input_shape)\n\nclass Encoder(EquivariantModule):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        # p4m\n        self.space = flipRot2dOnR2(4)\n\n        # self.conv1_rgb = ConvBlock(self.space, 3, 16)\n        # self.conv1_dem = ConvBlock(self.space, 1, 8)        \n        self.conv1_rgb = ConvBlock1(3, 16)\n        self.conv1_dem = ConvBlock1(1, 8)\n        self.conv2 = ConvBlock(self.space, 24, 32)\n        self.conv3 = ConvBlock(self.space, 32, 64)\n        self.conv4 = ConvBlock(self.space, 64, 128)\n        self.bottleneck = ConvBlock(self.space, 128, 256)\n\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        # fc\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x_rgb, x_dem):\n        c1_rgb = self.conv1_rgb(x_rgb)\n        c1_dem = self.conv1_dem(x_dem)\n        combined = torch.cat([c1_rgb, c1_dem], dim=1)\n\n        c2 = self.conv2(combined)\n        p2 = nn.MaxPool2d(kernel_size=2)(c2)#.tensor)\n\n        c3 = self.conv3(p2)\n        p3 = nn.MaxPool2d(kernel_size=2)(c3)#.tensor)\n\n        c4 = self.conv4(p3)\n        p4 = nn.MaxPool2d(kernel_size=2)(c4)#.tensor)\n\n        bn = self.bottleneck(p4)\n        pooled = self.global_pool(bn)#.tensor)\n        return self.fc(pooled)\n\n    def evaluate_output_shape(self, input_shape):\n        raise NotImplementedError(\"Shape evaluation not implemented yet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T05:45:06.140184Z","iopub.execute_input":"2025-01-12T05:45:06.140713Z","iopub.status.idle":"2025-01-12T05:45:07.705996Z","shell.execute_reply.started":"2025-01-12T05:45:06.140678Z","shell.execute_reply":"2025-01-12T05:45:07.704876Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.5175]], grad_fn=<SigmoidBackward0>)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# from torchinfo import summary\n\n# model = Encoder().to(device)\n# summary(model, input_size=[(1, 3, 256, 256), (1, 1, 256, 256)]) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T05:49:03.595290Z","iopub.execute_input":"2025-01-12T05:49:03.595885Z","iopub.status.idle":"2025-01-12T05:49:03.616300Z","shell.execute_reply.started":"2025-01-12T05:49:03.595847Z","shell.execute_reply":"2025-01-12T05:49:03.614880Z"}},"outputs":[{"name":"stdout","text":"Total parameters: 102137\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"data_dir = '/kaggle/input/bijie/Bijie_dataset'\nimages, dems, labels = load_real_data(data_dir, target_size=(256, 256))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T11:48:57.331365Z","iopub.execute_input":"2025-01-08T11:48:57.331721Z","iopub.status.idle":"2025-01-08T11:49:59.224459Z","shell.execute_reply.started":"2025-01-08T11:48:57.331665Z","shell.execute_reply":"2025-01-08T11:49:59.223785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(dems)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T11:50:18.166835Z","iopub.execute_input":"2025-01-08T11:50:18.167180Z","iopub.status.idle":"2025-01-08T11:50:18.173165Z","shell.execute_reply.started":"2025-01-08T11:50:18.167151Z","shell.execute_reply":"2025-01-08T11:50:18.172298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_img, X_test_img, X_train_dem, X_test_dem, y_train, y_test = train_test_split(\n    images, dems, labels, test_size=0.2, random_state=42\n)\n\nprint(X_train_img.shape, X_train_dem[0].shape, y_train[0].shape)\nprint(\"Training data size:\", len(X_train_img))\nprint(\"Testing data size:\", len(X_test_img))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T11:50:42.081924Z","iopub.execute_input":"2025-01-08T11:50:42.082288Z","iopub.status.idle":"2025-01-08T11:50:44.196851Z","shell.execute_reply.started":"2025-01-08T11:50:42.082259Z","shell.execute_reply":"2025-01-08T11:50:44.196065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, images, dems, masks):\n        self.images = images\n        self.dems = dems\n        self.masks = masks\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        return {\n            'image': self.images[idx],\n            'dem': self.dems[idx],\n            'label': self.masks[idx]\n        }\n\ntrain_dataset = CustomDataset(X_train_img, X_train_dem, y_train)\nval_dataset = CustomDataset(X_test_img, X_test_dem, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T11:55:23.296917Z","iopub.execute_input":"2025-01-08T11:55:23.297211Z","iopub.status.idle":"2025-01-08T11:55:23.303126Z","shell.execute_reply.started":"2025-01-08T11:55:23.297190Z","shell.execute_reply":"2025-01-08T11:55:23.302109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Encoder().to(device)\nepochs = 200\nbest_val_loss = float('inf')\nbest_model_path = 'best_unet_model.pth'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:03:57.662797Z","iopub.execute_input":"2025-01-08T12:03:57.663114Z","iopub.status.idle":"2025-01-08T12:03:57.683326Z","shell.execute_reply.started":"2025-01-08T12:03:57.663088Z","shell.execute_reply":"2025-01-08T12:03:57.682618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.MSELoss()\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n\n    for batch in train_loader:\n        images = batch['image'].to(device)\n        dems = batch['dem'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images, dems).squeeze(1) \n\n        binary_preds = (outputs > 0.5).float()\n\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    train_loss /= len(train_loader)\n\n    model.eval()\n    val_loss = 0.0\n    all_preds = []\n    all_targets = []\n    with torch.no_grad():\n        for batch in val_loader:\n            images = batch['image'].to(device)\n            dems = batch['dem'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(images, dems).squeeze(1)\n\n            binary_preds = (outputs > 0.5).float()\n\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            preds = binary_preds.cpu().numpy().astype(int)\n            targets = labels.cpu().numpy().astype(int)\n            all_preds.append(preds)\n            all_targets.append(targets)\n\n    val_loss /= len(val_loader)\n\n    all_preds = np.concatenate([pred.flatten() for pred in all_preds])\n    all_targets = np.concatenate([target.flatten() for target in all_targets])\n\n    accuracy = accuracy_score(all_targets, all_preds)\n    precision = precision_score(all_targets, all_preds, zero_division=0)\n    recall = recall_score(all_targets, all_preds, zero_division=0)\n    f1 = f1_score(all_targets, all_preds)\n    iou = jaccard_score(all_targets, all_preds)\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, IoU: {iou}, F1: {f1}\")\n\n    scheduler.step(val_loss)\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"Saved best model with val loss: {best_val_loss}\")\n\nprint(\"Training complete. Best model saved to\", best_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:14:12.325733Z","iopub.execute_input":"2025-01-08T12:14:12.326070Z","execution_failed":"2025-01-08T12:16:46.007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}