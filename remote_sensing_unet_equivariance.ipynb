{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f21d8657-b94b-4d13-8e6e-9d893eb8ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88219d44-4650-4447-8e49-72109fc105db",
   "metadata": {},
   "source": [
    "`Helper functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07f0578d-8a19-4181-b958-6ec42f617c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, target_size):\n",
    "    image = Image.open(image_path).convert('RGB' if target_size[2] == 3 else 'L')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size[:2]),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "def load_real_data(data_dir, target_size=(256, 256)):\n",
    "    landslide_dir = os.path.join(data_dir, 'landslide')\n",
    "    non_landslide_dir = os.path.join(data_dir, 'non-landslide')\n",
    "\n",
    "    images = []\n",
    "    dems = []\n",
    "    masks = []\n",
    "\n",
    "    # Load landslide data\n",
    "    for filename in os.listdir(os.path.join(landslide_dir, 'image')):\n",
    "        if filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(landslide_dir, 'image', filename)\n",
    "            dem_path = os.path.join(landslide_dir, 'dem', filename)\n",
    "            mask_path = os.path.join(landslide_dir, 'mask', filename)\n",
    "\n",
    "            image = load_image(image_path, target_size + (3,))  # RGB (C=3)\n",
    "            dem = load_image(dem_path, target_size + (1,))      # Grayscale (C=1)\n",
    "            mask = load_image(mask_path, target_size + (1,))    # Grayscale (C=1)\n",
    "\n",
    "            images.append(image)\n",
    "            dems.append(dem)\n",
    "            masks.append(mask)\n",
    "\n",
    "    # Load non-landslide data\n",
    "    for filename in os.listdir(os.path.join(non_landslide_dir, 'image')):\n",
    "        if filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(non_landslide_dir, 'image', filename)\n",
    "            dem_path = os.path.join(non_landslide_dir, 'dem', filename)\n",
    "\n",
    "            image = load_image(image_path, target_size + (3,))  # RGB (C=3)\n",
    "            dem = load_image(dem_path, target_size + (1,))      # Grayscale (C=1)\n",
    "            mask = torch.zeros((1, *target_size), dtype=torch.float32)  # Mask is all zeros, (C=1)\n",
    "\n",
    "            images.append(image)\n",
    "            dems.append(dem)\n",
    "            masks.append(mask)\n",
    "\n",
    "    # Stack tensors\n",
    "    images = torch.stack(images)\n",
    "    dems = torch.stack(dems)\n",
    "    masks = torch.stack(masks)\n",
    "\n",
    "    return images, dems, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee88b3-9554-42ff-adf1-99321430f254",
   "metadata": {},
   "source": [
    "`Network components`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d04895d2-319e-4a00-89e7-3134298e8a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.g1 = nn.Conv2d(in_channels, skip_channels, kernel_size=1)\n",
    "        self.x1 = nn.Conv2d(skip_channels, skip_channels, kernel_size=1)\n",
    "        self.psi = nn.Conv2d(skip_channels, 1, kernel_size=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # Upsample g1 to match the spatial dimensions of skip\n",
    "        g1 = self.upsample(self.g1(x))\n",
    "        x1 = self.x1(skip)\n",
    "        psi = self.relu(g1 + x1)  # Element-wise addition\n",
    "        psi = self.sigmoid(self.psi(psi))  # Generate attention weights\n",
    "        return skip * psi  # Element-wise multiplication\n",
    "\n",
    "class UpSampleConcat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UpSampleConcat, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        return torch.cat([x, skip], dim=1)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.conv1_rgb = ConvBlock(3, 16)\n",
    "        self.conv1_dem = ConvBlock(1, 8)\n",
    "        self.conv2 = ConvBlock(24, 32)\n",
    "        self.conv3 = ConvBlock(32, 64)\n",
    "        self.conv4 = ConvBlock(64, 128)\n",
    "        self.bottleneck = ConvBlock(128, 256)\n",
    "\n",
    "        self.att4 = AttentionBlock(256, 128)\n",
    "        self.up4 = UpSampleConcat()\n",
    "        self.conv5 = ConvBlock(384, 128)\n",
    "\n",
    "        self.att3 = AttentionBlock(128, 64)\n",
    "        self.up3 = UpSampleConcat()\n",
    "        self.conv6 = ConvBlock(192, 64)\n",
    "\n",
    "        self.att2 = AttentionBlock(64, 32)\n",
    "        self.up2 = UpSampleConcat()\n",
    "        self.conv7 = ConvBlock(96, 32)\n",
    "\n",
    "        self.conv8 = ConvBlock(56, 16)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(16, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_rgb, x_dem):\n",
    "        c1_rgb = self.conv1_rgb(x_rgb)\n",
    "        c1_dem = self.conv1_dem(x_dem)\n",
    "        combined = torch.cat([c1_rgb, c1_dem], dim=1)\n",
    "\n",
    "        c2 = self.conv2(combined)\n",
    "        p2 = nn.MaxPool2d(kernel_size=2)(c2)\n",
    "\n",
    "        c3 = self.conv3(p2)\n",
    "        p3 = nn.MaxPool2d(kernel_size=2)(c3)\n",
    "\n",
    "        c4 = self.conv4(p3)\n",
    "        p4 = nn.MaxPool2d(kernel_size=2)(c4)\n",
    "\n",
    "        bn = self.bottleneck(p4)\n",
    "\n",
    "        a4 = self.att4(bn, c4)\n",
    "        u4 = self.up4(bn, a4)\n",
    "        c5 = self.conv5(u4)\n",
    "\n",
    "        a3 = self.att3(c5, c3)\n",
    "        u3 = self.up3(c5, a3)\n",
    "        c6 = self.conv6(u3)\n",
    "\n",
    "        a2 = self.att2(c6, c2)\n",
    "        u2 = self.up2(c6, a2)\n",
    "        c7 = self.conv7(u2)\n",
    "\n",
    "        u1 = torch.cat([c7, combined], dim=1) # upsampling not needed here\n",
    "        c8 = self.conv8(u1)\n",
    "\n",
    "        return self.out(c8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3addf640-8b7d-45d8-bd98-ebd10c404be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2804da87-7e7c-4c6f-8fa1-71927657d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Bijie_dataset/Bijie_dataset'\n",
    "images, dems, masks = load_real_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07a81556-70c2-4b5a-a6e0-0c0180a8553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2218, 3, 256, 256]) torch.Size([1, 256, 256]) torch.Size([1, 256, 256])\n",
      "Training data size: 2218\n",
      "Testing data size: 555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train-test split\n",
    "X_train_img, X_test_img, X_train_dem, X_test_dem, y_train, y_test = train_test_split(\n",
    "    images, dems, masks, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(X_train_img.shape, X_train_dem[0].shape, y_train[0].shape)\n",
    "\n",
    "# ####### Using only 10% of data locally\n",
    "# def subset_data(X_img, X_dem, y, fraction=0.1):\n",
    "#     subset_size = int(len(X_img) * fraction)\n",
    "#     return X_img[:subset_size], X_dem[:subset_size], y[:subset_size]\n",
    "\n",
    "# X_train_img, X_train_dem, y_train = subset_data(X_train_img, X_train_dem, y_train, fraction=0.1)\n",
    "# X_test_img, X_test_dem, y_test = subset_data(X_test_img, X_test_dem, y_test, fraction=0.1)\n",
    "\n",
    "print(\"Training data size:\", len(X_train_img))\n",
    "print(\"Testing data size:\", len(X_test_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8216a4fb-7d6e-4d72-b4fd-703f76d2ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, dems, masks):\n",
    "        self.images = images\n",
    "        self.dems = dems\n",
    "        self.masks = masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'image': self.images[idx],\n",
    "            'dem': self.dems[idx],\n",
    "            'mask': self.masks[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = CustomDataset(X_train_img, X_train_dem, y_train)\n",
    "val_dataset = CustomDataset(X_test_img, X_test_dem, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38800797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().to(device)\n",
    "epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = 'best_unet_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "976bd5cd-f63b-487b-a00b-b3514170b1a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 21\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Validation loop\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        dems = batch['dem'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, dems).squeeze(1)\n",
    "        loss = criterion(outputs.unsqueeze(1), masks)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            dems = batch['dem'].to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "\n",
    "            outputs = model(images, dems).squeeze(1)\n",
    "            loss = criterion(outputs.unsqueeze(1), masks)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and targets for metric calculation\n",
    "            preds = (outputs > 0.5).cpu().numpy().astype(int)  # Thresholding at 0.5\n",
    "            targets = masks.cpu().numpy().astype(int)\n",
    "\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(targets)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Flatten lists for metric calculations\n",
    "    all_preds = np.concatenate([pred.flatten() for pred in all_preds])\n",
    "    all_targets = np.concatenate([target.flatten() for target in all_targets])\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    recall = recall_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    iou = jaccard_score(all_targets, all_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}, \"\n",
    "          f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}, IoU: {iou}\")\n",
    "\n",
    "    # Save the model if val loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Saved best model with val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete. Best model saved to\", best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e6554fb-ee0d-4e52-b3dc-b4e91b1e30e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "UNet                                     [1, 1, 256, 256]          --\n",
       "├─ConvBlock: 1-1                         [1, 16, 256, 256]         --\n",
       "│    └─Sequential: 2-1                   [1, 16, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-1                  [1, 16, 256, 256]         448\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 16, 256, 256]         32\n",
       "│    │    └─ReLU: 3-3                    [1, 16, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-4                  [1, 16, 256, 256]         2,320\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 16, 256, 256]         32\n",
       "│    │    └─ReLU: 3-6                    [1, 16, 256, 256]         --\n",
       "├─ConvBlock: 1-2                         [1, 8, 256, 256]          --\n",
       "│    └─Sequential: 2-2                   [1, 8, 256, 256]          --\n",
       "│    │    └─Conv2d: 3-7                  [1, 8, 256, 256]          80\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 8, 256, 256]          16\n",
       "│    │    └─ReLU: 3-9                    [1, 8, 256, 256]          --\n",
       "│    │    └─Conv2d: 3-10                 [1, 8, 256, 256]          584\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 8, 256, 256]          16\n",
       "│    │    └─ReLU: 3-12                   [1, 8, 256, 256]          --\n",
       "├─ConvBlock: 1-3                         [1, 32, 256, 256]         --\n",
       "│    └─Sequential: 2-3                   [1, 32, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-13                 [1, 32, 256, 256]         6,944\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 32, 256, 256]         64\n",
       "│    │    └─ReLU: 3-15                   [1, 32, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-16                 [1, 32, 256, 256]         9,248\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 32, 256, 256]         64\n",
       "│    │    └─ReLU: 3-18                   [1, 32, 256, 256]         --\n",
       "├─ConvBlock: 1-4                         [1, 64, 128, 128]         --\n",
       "│    └─Sequential: 2-4                   [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-19                 [1, 64, 128, 128]         18,496\n",
       "│    │    └─BatchNorm2d: 3-20            [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-21                   [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-22                 [1, 64, 128, 128]         36,928\n",
       "│    │    └─BatchNorm2d: 3-23            [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-24                   [1, 64, 128, 128]         --\n",
       "├─ConvBlock: 1-5                         [1, 128, 64, 64]          --\n",
       "│    └─Sequential: 2-5                   [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-25                 [1, 128, 64, 64]          73,856\n",
       "│    │    └─BatchNorm2d: 3-26            [1, 128, 64, 64]          256\n",
       "│    │    └─ReLU: 3-27                   [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-28                 [1, 128, 64, 64]          147,584\n",
       "│    │    └─BatchNorm2d: 3-29            [1, 128, 64, 64]          256\n",
       "│    │    └─ReLU: 3-30                   [1, 128, 64, 64]          --\n",
       "├─ConvBlock: 1-6                         [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-6                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-31                 [1, 256, 32, 32]          295,168\n",
       "│    │    └─BatchNorm2d: 3-32            [1, 256, 32, 32]          512\n",
       "│    │    └─ReLU: 3-33                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-34                 [1, 256, 32, 32]          590,080\n",
       "│    │    └─BatchNorm2d: 3-35            [1, 256, 32, 32]          512\n",
       "│    │    └─ReLU: 3-36                   [1, 256, 32, 32]          --\n",
       "├─AttentionBlock: 1-7                    [1, 128, 64, 64]          --\n",
       "│    └─Conv2d: 2-7                       [1, 128, 32, 32]          32,896\n",
       "│    └─Upsample: 2-8                     [1, 128, 64, 64]          --\n",
       "│    └─Conv2d: 2-9                       [1, 128, 64, 64]          16,512\n",
       "│    └─ReLU: 2-10                        [1, 128, 64, 64]          --\n",
       "│    └─Conv2d: 2-11                      [1, 1, 64, 64]            129\n",
       "│    └─Sigmoid: 2-12                     [1, 1, 64, 64]            --\n",
       "├─UpSampleConcat: 1-8                    [1, 384, 64, 64]          --\n",
       "│    └─Upsample: 2-13                    [1, 256, 64, 64]          --\n",
       "├─ConvBlock: 1-9                         [1, 128, 64, 64]          --\n",
       "│    └─Sequential: 2-14                  [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-37                 [1, 128, 64, 64]          442,496\n",
       "│    │    └─BatchNorm2d: 3-38            [1, 128, 64, 64]          256\n",
       "│    │    └─ReLU: 3-39                   [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-40                 [1, 128, 64, 64]          147,584\n",
       "│    │    └─BatchNorm2d: 3-41            [1, 128, 64, 64]          256\n",
       "│    │    └─ReLU: 3-42                   [1, 128, 64, 64]          --\n",
       "├─AttentionBlock: 1-10                   [1, 64, 128, 128]         --\n",
       "│    └─Conv2d: 2-15                      [1, 64, 64, 64]           8,256\n",
       "│    └─Upsample: 2-16                    [1, 64, 128, 128]         --\n",
       "│    └─Conv2d: 2-17                      [1, 64, 128, 128]         4,160\n",
       "│    └─ReLU: 2-18                        [1, 64, 128, 128]         --\n",
       "│    └─Conv2d: 2-19                      [1, 1, 128, 128]          65\n",
       "│    └─Sigmoid: 2-20                     [1, 1, 128, 128]          --\n",
       "├─UpSampleConcat: 1-11                   [1, 192, 128, 128]        --\n",
       "│    └─Upsample: 2-21                    [1, 128, 128, 128]        --\n",
       "├─ConvBlock: 1-12                        [1, 64, 128, 128]         --\n",
       "│    └─Sequential: 2-22                  [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-43                 [1, 64, 128, 128]         110,656\n",
       "│    │    └─BatchNorm2d: 3-44            [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-45                   [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-46                 [1, 64, 128, 128]         36,928\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-48                   [1, 64, 128, 128]         --\n",
       "├─AttentionBlock: 1-13                   [1, 32, 256, 256]         --\n",
       "│    └─Conv2d: 2-23                      [1, 32, 128, 128]         2,080\n",
       "│    └─Upsample: 2-24                    [1, 32, 256, 256]         --\n",
       "│    └─Conv2d: 2-25                      [1, 32, 256, 256]         1,056\n",
       "│    └─ReLU: 2-26                        [1, 32, 256, 256]         --\n",
       "│    └─Conv2d: 2-27                      [1, 1, 256, 256]          33\n",
       "│    └─Sigmoid: 2-28                     [1, 1, 256, 256]          --\n",
       "├─UpSampleConcat: 1-14                   [1, 96, 256, 256]         --\n",
       "│    └─Upsample: 2-29                    [1, 64, 256, 256]         --\n",
       "├─ConvBlock: 1-15                        [1, 32, 256, 256]         --\n",
       "│    └─Sequential: 2-30                  [1, 32, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-49                 [1, 32, 256, 256]         27,680\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 32, 256, 256]         64\n",
       "│    │    └─ReLU: 3-51                   [1, 32, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-52                 [1, 32, 256, 256]         9,248\n",
       "│    │    └─BatchNorm2d: 3-53            [1, 32, 256, 256]         64\n",
       "│    │    └─ReLU: 3-54                   [1, 32, 256, 256]         --\n",
       "├─LastUpSampleConcat: 1-16               [1, 56, 256, 256]         --\n",
       "├─ConvBlock: 1-17                        [1, 16, 256, 256]         --\n",
       "│    └─Sequential: 2-31                  [1, 16, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-55                 [1, 16, 256, 256]         8,080\n",
       "│    │    └─BatchNorm2d: 3-56            [1, 16, 256, 256]         32\n",
       "│    │    └─ReLU: 3-57                   [1, 16, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-58                 [1, 16, 256, 256]         2,320\n",
       "│    │    └─BatchNorm2d: 3-59            [1, 16, 256, 256]         32\n",
       "│    │    └─ReLU: 3-60                   [1, 16, 256, 256]         --\n",
       "├─Sequential: 1-18                       [1, 1, 256, 256]          --\n",
       "│    └─Conv2d: 2-32                      [1, 1, 256, 256]          17\n",
       "│    └─Sigmoid: 2-33                     [1, 1, 256, 256]          --\n",
       "==========================================================================================\n",
       "Total params: 2,034,908\n",
       "Trainable params: 2,034,908\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 12.26\n",
       "==========================================================================================\n",
       "Input size (MB): 1.05\n",
       "Forward/backward pass size (MB): 365.07\n",
       "Params size (MB): 8.14\n",
       "Estimated Total Size (MB): 374.26\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = UNet().to(device)\n",
    "summary(model, input_size=[(1, 3, 256, 256), (1, 1, 256, 256)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362170b4-d6f2-491a-8635-c8abef0dab98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
